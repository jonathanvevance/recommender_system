{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "curdir = os.getcwd()\n",
    "prevdir = os.path.dirname(curdir)\n",
    "\n",
    "folder = prevdir + '/datasets/ml-100k/'\n",
    "rating_cols = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "ratings = pd.read_csv(folder + 'u.data', sep = '\\t', names = rating_cols, encoding = 'latin-1')\n",
    "ratings.drop('timestamp', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_rated = 3\n",
    "value_counts = ratings['movie_id'].value_counts()\n",
    "remove_movies = value_counts.loc[value_counts < min_rated].index\n",
    "ratings = ratings.loc[~ ratings['movie_id'].isin(remove_movies)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful article on data splitting: https://stackoverflow.com/questions/43129764/splitting-data-set-into-training-and-testing-sets-on-recommender-systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_dataset(ratings):\n",
    "\n",
    "    users = set(ratings['user_id'].unique())\n",
    "    movie_counts = ratings['movie_id'].value_counts()\n",
    "    ratings = ratings.sort_values(by = 'movie_id').reset_index(drop = True)\n",
    "    rem_ratings = ratings.copy()\n",
    "\n",
    "    idx = 0\n",
    "    count = 0\n",
    "    X_test = None\n",
    "    X_train = None\n",
    "    X_valid = None\n",
    "    while idx < ratings.shape[0]:\n",
    "        \n",
    "        print(f'\\r{idx} of {ratings.shape[0] - 1}', end = \" \", sep = \" \")\n",
    "        \n",
    "        if count == 0:\n",
    "            if not isinstance(X_train, pd.DataFrame):\n",
    "                first_row = ratings.loc[idx].to_dict()\n",
    "                X_train = pd.DataFrame(first_row, index = [0])     \n",
    "            else:\n",
    "                X_train = X_train.append(ratings.loc[idx], ignore_index = True)\n",
    "            rem_ratings.drop(idx, inplace = True)\n",
    "\n",
    "            idx += 1\n",
    "            count = 1\n",
    "               \n",
    "        elif count == 1:\n",
    "            if not isinstance(X_valid, pd.DataFrame):\n",
    "                first_row = ratings.loc[idx].to_dict()\n",
    "                X_valid = pd.DataFrame(first_row, index = [0])\n",
    "            else:\n",
    "                X_valid = X_valid.append(ratings.loc[idx], ignore_index = True)\n",
    "            rem_ratings.drop(idx, inplace = True)\n",
    "            \n",
    "            idx += 1\n",
    "            count = 2\n",
    "            \n",
    "        elif count == 2:\n",
    "            if not isinstance(X_test, pd.DataFrame):\n",
    "                first_row = ratings.loc[idx].to_dict()\n",
    "                X_test = pd.DataFrame(first_row, index = [0])\n",
    "            else:\n",
    "                X_test = X_test.append(ratings.loc[idx], ignore_index = True)\n",
    "            \n",
    "            count = 0\n",
    "            rem_ratings.drop(idx, inplace = True)\n",
    "            movie_id = ratings.loc[idx, 'movie_id'] \n",
    "            assert movie_counts[movie_id] > 2\n",
    "            idx += (movie_counts[movie_id] - 2)\n",
    "    \n",
    "    train, test= train_test_split(rem_ratings, \n",
    "                                  test_size = 0.2, \n",
    "                                  random_state = 42)\n",
    "    \n",
    "    X_test = pd.concat([X_test, test])\n",
    "    X_train = pd.concat([X_train, train])\n",
    "    \n",
    "    users_test = X_test['user_id'].unique()\n",
    "    users_valid = X_valid['user_id'].unique()\n",
    "    users_train = X_train['user_id'].unique()\n",
    "    \n",
    "    rem_users_test = list(users.difference(set(users_test)))\n",
    "    rem_users_valid = list(users.difference(set(users_valid)))\n",
    "    rem_users_train = list(users.difference(set(users_train)))\n",
    "\n",
    "    for user_id in rem_users_test:\n",
    "        row = ratings.loc[ratings['user_id'] == user_id].sample()\n",
    "        X_test = X_test.append(row, ignore_index = True)\n",
    "        \n",
    "    for user_id in rem_users_valid:\n",
    "        row = ratings.loc[ratings['user_id'] == user_id].sample()\n",
    "        X_valid = X_valid.append(row, ignore_index = True)\n",
    "        \n",
    "    for user_id in rem_users_train:\n",
    "        row = ratings.loc[ratings['user_id'] == user_id].sample()\n",
    "        X_train = X_train.append(row, ignore_index = True)\n",
    "    \n",
    "    return X_train, X_valid, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = X_train.pivot_table(values = 'rating', index = 'user_id', columns = 'movie_id').to_numpy()\n",
    "matrix_valid = X_valid.pivot_table(values = 'rating', index = 'user_id', columns = 'movie_id').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def score(cf_model):\n",
    "    id_pairs = zip(X_test['user_id'], X_test['movie_id'])\n",
    "    y_pred = np.array([cf_model(user, movie) for (user, movie) in id_pairs])\n",
    "    y_true = np.array(X_test['rating'])\n",
    "    return rmse(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_statusbar(iteration, total, losses_list, metrics=None):\n",
    "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result()) for m in losses_list + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} - \".format(iteration, total) + metrics, end = end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def random_batch(matrix, filled_idx_tuples, batch_size = None):\n",
    "    \n",
    "    user_batch = None\n",
    "    item_batch = None\n",
    "    ratings_batch = None\n",
    "    \n",
    "    if batch_size == None:\n",
    "        indices = range(len(filled_idx_tuples))\n",
    "    else:\n",
    "        indices = np.random.randint(len(filled_idx_tuples), size = batch_size)\n",
    "    \n",
    "    for i, rdm_idx in enumerate(indices):\n",
    "        (user_idx, item_idx) = filled_idx_tuples[rdm_idx]\n",
    "\n",
    "        if i == 0:\n",
    "            user_batch = matrix[user_idx, :]\n",
    "            item_batch = matrix[:, item_idx]\n",
    "            ratings_batch = matrix[user_idx, item_idx]\n",
    "        else:        \n",
    "            user_batch = np.vstack((user_batch, matrix[user_idx, :]))\n",
    "            item_batch = np.vstack((item_batch, matrix[:, item_idx]))\n",
    "            ratings_batch = np.vstack((ratings_batch, matrix[user_idx, item_idx]))\n",
    "            \n",
    "    return ((user_batch, item_batch), ratings_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_valid = np.where(~ np.isnan(matrix_valid))\n",
    "filled_idx_tuples_valid = list(zip(*idx_valid))\n",
    "validation_set = random_batch(matrix_valid, filled_idx_tuples_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMFLayer(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        assert(input_shape[0][1] == input_shape[1][1]) \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        user_vec, item_vec = inputs\n",
    "        elem_product = tf.math.multiply(user_vec, item_vec)\n",
    "        return elem_product \n",
    "    \n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def sigmoid_1_5(z):\n",
    "    return 1 + (4 * tf.math.sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LeakyReLU\n",
    "\n",
    "class DenseBlock(keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_units, regularizer = None, leaky_alpha = 0.3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = []\n",
    "        for __ in range(n_layers):\n",
    "            self.hidden.append(Dense(n_units, kernel_regularizer = regularizer))\n",
    "            self.hidden.append(LeakyReLU(alpha = leaky_alpha))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return Z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, LeakyReLU\n",
    "\n",
    "class NeuMFModel(keras.models.Model):\n",
    "    def __init__(self, n_dense_block = 4, n_units = 30, regul = None, leaky_apha = 0.3, **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        self.n_units = n_units\n",
    "        \n",
    "        self.mf_user_vec = Dense(n_units, kernel_regularizer = regul, name = 'mf_user_vec') #\n",
    "        self.leaky_mf_user_vec = LeakyReLU(alpha = leaky_alpha)\n",
    "        \n",
    "        self.mf_item_vec = Dense(n_units, kernel_regularizer = regul, name = 'mf_item_vec') #\n",
    "        self.leaky_mf_item_vec = LeakyReLU(alpha = leaky_alpha)\n",
    "        \n",
    "        self.mlp_user_vec = Dense(n_units, kernel_regularizer = regul, name = 'mlp_user_vec') #\n",
    "        self.leaky_mlp_user_vec = LeakyReLU(alpha = leaky_alpha)\n",
    "        \n",
    "        self.mlp_item_vec = Dense(n_units, kernel_regularizer = regul, name = 'mlp_item_vec') #\n",
    "        self.leaky_mlp_item_vec = LeakyReLU(alpha = leaky_alpha)\n",
    "\n",
    "        self.concat_mlp_layer = keras.layers.Concatenate(name = 'concat_mlp_layer')\n",
    "\n",
    "        self.gmf_layer = GMFLayer(name = 'gmf_layer')\n",
    "        self.mlp_block = DenseBlock(n_dense_block, n_units, regularizer = regul, name = 'mlp_block')\n",
    "        \n",
    "        self.neu_mf_layer = keras.layers.Concatenate(name = 'neu_mf_layer')\n",
    "        self.rating_output = Dense(1, activation = sigmoid_1_5, \n",
    "                                   kernel_regularizer = regul, name = 'output')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        user_input, item_input = inputs        \n",
    "        mf_user_vec = self.mf_user_vec(user_input)\n",
    "        mf_user_vec_act = self.leaky_mf_user_vec(mf_user_vec)\n",
    "        \n",
    "        mf_item_vec = self.mf_item_vec(item_input)\n",
    "        mf_item_vec_act = self.leaky_mf_item_vec(mf_item_vec)\n",
    "        \n",
    "        mlp_user_vec = self.mlp_user_vec(user_input)\n",
    "        mlp_user_vec_act = self.leaky_mlp_user_vec(mlp_user_vec)\n",
    "        \n",
    "        mlp_item_vec = self.mlp_item_vec(item_input)\n",
    "        mlp_item_vec_act = self.leaky_mlp_item_vec(mlp_item_vec)\n",
    "        \n",
    "        concat_mlp_layer = self.concat_mlp_layer([mlp_user_vec_act, mlp_item_vec_act])\n",
    "        mlp_block = self.mlp_block(concat_mlp_layer)\n",
    "        gmf_layer = self.gmf_layer([mf_user_vec_act, mf_item_vec_act])\n",
    "        neu_mf_layer = self.neu_mf_layer([gmf_layer, mlp_block])\n",
    "        rating_output = self.rating_output(neu_mf_layer)\n",
    "        return rating_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def training_loop(model, matrix, validation_set, optimizer, n_epochs, batch_size = 64):\n",
    "    \n",
    "    indices = np.where(~ np.isnan(matrix))\n",
    "    filled_idx_tuples = list(zip(*indices))\n",
    "    matrix = np.nan_to_num(matrix)\n",
    "    \n",
    "    X_valid, y_valid = validation_set\n",
    "    user_valid_set, movie_valid_set = X_valid\n",
    "    user_valid_set = np.nan_to_num(user_valid_set)\n",
    "    movie_valid_set = np.nan_to_num(movie_valid_set)\n",
    "    \n",
    "    n_steps = len(filled_idx_tuples) // batch_size\n",
    "    loss_func = keras.losses.MeanSquaredError()\n",
    "    metrics = [keras.metrics.MeanSquaredError(name = 'validation mse')]\n",
    "    mean_loss = keras.metrics.Mean(name = 'train mse')\n",
    "    mean_total_loss = keras.metrics.Mean(name = 'train mse + reg')\n",
    "        \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "        for step in range(1, n_steps + 1):\n",
    "            input_batch, output_batch = random_batch(matrix, filled_idx_tuples, batch_size)\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = model(input_batch)\n",
    "                loss = tf.reduce_mean(loss_func(output_batch, y_pred))\n",
    "                total_loss = tf.add_n([loss] + model.losses)\n",
    "    \n",
    "            gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            \n",
    "            mean_loss(loss)\n",
    "            mean_total_loss(total_loss)\n",
    "            \n",
    "            y_pred_valid = model((user_valid_set, movie_valid_set))\n",
    "            for idx, metric in enumerate(metrics):\n",
    "                if idx < len(metrics) - 1:\n",
    "                    metric(output_batch, y_pred)\n",
    "                else: \n",
    "                    metric(y_valid, y_pred_valid) \n",
    "            \n",
    "            print_statusbar(step * batch_size, len(filled_idx_tuples), [mean_loss, mean_total_loss], metrics)\n",
    "            \n",
    "        print_statusbar(len(filled_idx_tuples), len(filled_idx_tuples), [mean_loss, mean_total_loss], metrics)\n",
    "        for metric in [mean_loss, mean_total_loss] + metrics:\n",
    "            metric.reset_states()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_alpha = 0.4\n",
    "reg = keras.regularizers.l2(0.1)\n",
    "opt = keras.optimizers.Adam(0.0001)\n",
    "\n",
    "model = NeuMFModel(name = 'neu_mf_model', n_dense_block = 5, n_units = 30, \n",
    "                   regul = reg, leaky_apha = leaky_alpha)\n",
    "\n",
    "training_loop(model, matrix, validation_set, optimizer = opt, n_epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try to overfit using a small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = ratings['user_id'].unique()\n",
    "movies = ratings['movie_id'].unique()\n",
    "\n",
    "perc = 0.001\n",
    "users_sub = np.random.choice(users, int(len(users) * perc), replace = False)\n",
    "movies_sub = np.random.choice(movies, int(len(movies) * perc), replace = False)\n",
    "\n",
    "filter_ = (ratings['user_id'].isin(users_sub)) | (ratings['movie_id'].isin(movies_sub)) \n",
    "ratings_sub = ratings.loc[filter_]\n",
    "\n",
    "min_rated = 3\n",
    "value_counts = ratings_sub['movie_id'].value_counts()\n",
    "remove_movies = value_counts.loc[value_counts < min_rated].index\n",
    "ratings_sub = ratings_sub.loc[~ ratings_sub['movie_id'].isin(remove_movies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0 of 190 \r",
      "1 of 190 \r",
      "2 of 190 "
     ]
    }
   ],
   "source": [
    "X_train_sub, X_valid_sub, X_test_sub = split_dataset(ratings_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_sub = X_train_sub.pivot_table(values = 'rating', index = 'user_id', columns = 'movie_id').to_numpy()\n",
    "matrix_valid_sub = X_valid_sub.pivot_table(values = 'rating', index = 'user_id', columns = 'movie_id').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_valid_sub = np.where(~ np.isnan(matrix_valid_sub))\n",
    "filled_idx_tuples_valid_sub = list(zip(*idx_valid_sub))\n",
    "validation_set_sub = random_batch(matrix_valid_sub, filled_idx_tuples_valid_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_alpha = 0.4\n",
    "reg = keras.regularizers.l2(0.1)\n",
    "opt = keras.optimizers.Adam(0.0001)\n",
    "\n",
    "model = NeuMFModel(name = 'neu_mf_model', n_dense_block = 5, n_units = 30, \n",
    "                   regul = reg, leaky_apha = leaky_alpha)\n",
    "\n",
    "training_loop(model, matrix_sub, validation_set_sub, optimizer = opt, n_epochs = 1020)\n",
    "\n",
    "# Result: we are able to overfit a small dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check loss from random inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = 250\n",
    "n_movies = 300\n",
    "train_perc_nan = 0.99\n",
    "valid_perc_nan = 0.999\n",
    "\n",
    "matrix_rdm = 5 * np.random.rand(n_users, n_movies)\n",
    "matrix_rdm.ravel()[(\n",
    "    np.random.choice(matrix_rdm.size, int(n_movies * n_users * train_perc_nan), replace = False))] = np.nan\n",
    "\n",
    "matrix_valid_rdm = 5 * np.random.rand(n_users, n_movies)\n",
    "matrix_valid_rdm.ravel()[(\n",
    "    np.random.choice(matrix_valid_rdm.size, int(n_movies * n_users * valid_perc_nan), replace = False))] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_valid_rdm = np.where(~ np.isnan(matrix_valid_rdm))\n",
    "filled_idx_tuples_valid_rdm = list(zip(*idx_valid_rdm))\n",
    "validation_set_rdm = random_batch(matrix_valid_rdm, filled_idx_tuples_valid_rdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "750/750 - train mse: 2.3224 - train mse + reg: 39.6613 - validation mse: 2.2497\n",
      "Epoch 2/20\n",
      "750/750 - train mse: 2.5132 - train mse + reg: 39.1978 - validation mse: 2.2424\n",
      "Epoch 3/20\n",
      "750/750 - train mse: 2.3061 - train mse + reg: 38.3481 - validation mse: 2.2350\n",
      "Epoch 4/20\n",
      "750/750 - train mse: 2.3594 - train mse + reg: 37.7692 - validation mse: 2.2285\n",
      "Epoch 5/20\n",
      "750/750 - train mse: 2.3485 - train mse + reg: 37.1371 - validation mse: 2.2219\n",
      "Epoch 6/20\n",
      "750/750 - train mse: 2.2244 - train mse + reg: 36.4030 - validation mse: 2.2152\n",
      "Epoch 7/20\n",
      "750/750 - train mse: 2.3235 - train mse + reg: 35.9028 - validation mse: 2.2085\n",
      "Epoch 8/20\n",
      "750/750 - train mse: 2.3679 - train mse + reg: 35.3589 - validation mse: 2.2014\n",
      "Epoch 9/20\n",
      "750/750 - train mse: 2.2578 - train mse + reg: 34.6712 - validation mse: 2.1945\n",
      "Epoch 10/20\n",
      "750/750 - train mse: 2.1991 - train mse + reg: 34.0455 - validation mse: 2.1876\n",
      "Epoch 11/20\n",
      "750/750 - train mse: 2.3279 - train mse + reg: 33.6170 - validation mse: 2.1814\n",
      "Epoch 12/20\n",
      "750/750 - train mse: 2.2234 - train mse + reg: 32.9652 - validation mse: 2.1752\n",
      "Epoch 13/20\n",
      "750/750 - train mse: 2.1770 - train mse + reg: 32.3814 - validation mse: 2.1693\n",
      "Epoch 14/20\n",
      "750/750 - train mse: 2.3512 - train mse + reg: 32.0281 - validation mse: 2.1633\n",
      "Epoch 15/20\n",
      "750/750 - train mse: 2.1387 - train mse + reg: 31.2977 - validation mse: 2.1575\n",
      "Epoch 16/20\n",
      "750/750 - train mse: 2.2043 - train mse + reg: 30.8542 - validation mse: 2.1521\n",
      "Epoch 17/20\n",
      "750/750 - train mse: 2.2906 - train mse + reg: 30.4408 - validation mse: 2.1465\n",
      "Epoch 18/20\n",
      "750/750 - train mse: 2.2361 - train mse + reg: 29.8956 - validation mse: 2.1405\n",
      "Epoch 19/20\n",
      "750/750 - train mse: 2.2220 - train mse + reg: 29.3994 - validation mse: 2.1354\n",
      "Epoch 20/20\n",
      "750/750 - train mse: 2.1717 - train mse + reg: 28.8758 - validation mse: 2.1304\n"
     ]
    }
   ],
   "source": [
    "leaky_alpha = 0.4\n",
    "reg = keras.regularizers.l2(0.1)\n",
    "opt = keras.optimizers.Adam(0.0001)\n",
    "\n",
    "model = NeuMFModel(name = 'neu_mf_model', n_dense_block = 5, n_units = 30, \n",
    "                   regul = reg, leaky_apha = leaky_alpha)\n",
    "\n",
    "training_loop(model, matrix_rdm, validation_set_rdm, optimizer = opt, n_epochs = 20)\n",
    "# Result: dataset is informative since loss of random noise is high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try simpler NN models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted combination of element-wise product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = ratings['user_id'].unique()\n",
    "movies = ratings['movie_id'].unique()\n",
    "\n",
    "perc = 1\n",
    "users_sub = np.random.choice(users, int(len(users) * perc), replace = False)\n",
    "movies_sub = np.random.choice(movies, int(len(movies) * perc), replace = False)\n",
    "\n",
    "filter_ = (ratings['user_id'].isin(users_sub)) | (ratings['movie_id'].isin(movies_sub)) \n",
    "ratings_sub = ratings.loc[filter_]\n",
    "\n",
    "min_rated = 3\n",
    "value_counts = ratings_sub['movie_id'].value_counts()\n",
    "remove_movies = value_counts.loc[value_counts < min_rated].index\n",
    "ratings_sub = ratings_sub.loc[~ ratings_sub['movie_id'].isin(remove_movies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99721 of 99722                                                                                                                                                             "
     ]
    }
   ],
   "source": [
    "X_train_sub, X_valid_sub, X_test_sub = split_dataset(ratings_sub)\n",
    "\n",
    "matrix_sub = X_train_sub.pivot_table(values = 'rating', index = 'user_id', columns = 'movie_id').to_numpy()\n",
    "matrix_valid_sub = X_valid_sub.pivot_table(values = 'rating', index = 'user_id', columns = 'movie_id').to_numpy()\n",
    "\n",
    "idx_valid_sub = np.where(~ np.isnan(matrix_valid_sub))\n",
    "filled_idx_tuples_valid_sub = list(zip(*idx_valid_sub))\n",
    "validation_set_sub = random_batch(matrix_valid_sub, filled_idx_tuples_valid_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, LeakyReLU\n",
    "\n",
    "class NeuCFModel(keras.models.Model):\n",
    "    def __init__(self, n_dense_block = 4, n_units = 30, regul = None, leaky_apha = 0.3, **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        self.n_units = n_units\n",
    "        \n",
    "        self.mf_user_vec = Dense(n_units, kernel_regularizer = regul, name = 'mf_user_vec') #\n",
    "        self.leaky_mf_user_vec = LeakyReLU(alpha = leaky_alpha)\n",
    "        \n",
    "        self.mf_item_vec = Dense(n_units, kernel_regularizer = regul, name = 'mf_item_vec') #\n",
    "        self.leaky_mf_item_vec = LeakyReLU(alpha = leaky_alpha)\n",
    "        \n",
    "        self.gmf_layer = GMFLayer(name = 'gmf_layer')\n",
    "        self.rating_output = Dense(1, activation = sigmoid_1_5, kernel_regularizer = regul, name = 'output')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        user_input, item_input = inputs        \n",
    "        mf_user_vec = self.mf_user_vec(user_input)\n",
    "        mf_user_vec_act = self.leaky_mf_user_vec(mf_user_vec)\n",
    "        \n",
    "        mf_item_vec = self.mf_item_vec(item_input)\n",
    "        mf_item_vec_act = self.leaky_mf_item_vec(mf_item_vec)\n",
    "        \n",
    "        gmf_layer = self.gmf_layer([mf_user_vec_act, mf_item_vec_act])\n",
    "        rating_output = self.rating_output(gmf_layer)\n",
    "        return rating_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_alpha = 0.4\n",
    "reg = keras.regularizers.l2(0.1)\n",
    "opt = keras.optimizers.Adam(0.0001)\n",
    "\n",
    "model = NeuCFModel(name = 'neu_cf_model', n_dense_block = 5, n_units = 30, \n",
    "                   regul = reg, leaky_apha = leaky_alpha)\n",
    "\n",
    "training_loop(model, matrix_sub, validation_set_sub, optimizer = opt, n_epochs = 20)\n",
    "# Result: effective at learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = ratings['user_id'].unique()\n",
    "movies = ratings['movie_id'].unique()\n",
    "\n",
    "perc = 1\n",
    "users_sub = np.random.choice(users, int(len(users) * perc), replace = False)\n",
    "movies_sub = np.random.choice(movies, int(len(movies) * perc), replace = False)\n",
    "\n",
    "filter_ = (ratings['user_id'].isin(users_sub)) | (ratings['movie_id'].isin(movies_sub)) \n",
    "ratings_sub = ratings.loc[filter_]\n",
    "\n",
    "min_rated = 3\n",
    "value_counts = ratings_sub['movie_id'].value_counts()\n",
    "remove_movies = value_counts.loc[value_counts < min_rated].index\n",
    "ratings_sub = ratings_sub.loc[~ ratings_sub['movie_id'].isin(remove_movies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1814 of 1814       "
     ]
    }
   ],
   "source": [
    "X_train_sub, X_valid_sub, X_test_sub = split_dataset(ratings_sub)\n",
    "\n",
    "matrix_sub = X_train_sub.pivot_table(values = 'rating', index = 'user_id', columns = 'movie_id').to_numpy()\n",
    "matrix_valid_sub = X_valid_sub.pivot_table(values = 'rating', index = 'user_id', columns = 'movie_id').to_numpy()\n",
    "\n",
    "idx_valid_sub = np.where(~ np.isnan(matrix_valid_sub))\n",
    "filled_idx_tuples_valid_sub = list(zip(*idx_valid_sub))\n",
    "validation_set_sub = random_batch(matrix_valid_sub, filled_idx_tuples_valid_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LeakyReLU\n",
    "\n",
    "class DenseBlock(keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_units, regularizer = None, leaky_alpha = 0.3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = []\n",
    "        for __ in range(n_layers):\n",
    "            self.hidden.append(Dense(n_units, kernel_regularizer = regularizer))\n",
    "            self.hidden.append(LeakyReLU(alpha = leaky_alpha))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return Z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, LeakyReLU\n",
    "\n",
    "class EncoderMLP(keras.models.Model):\n",
    "    def __init__(self, n_dense_block = 4, n_units = 30, regul = None, leaky_apha = 0.3, **kwargs):\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        self.n_units = n_units\n",
    "        \n",
    "        self.mlp_user_vec = Dense(n_units, kernel_regularizer = regul, name = 'mlp_user_vec') #\n",
    "        self.leaky_mlp_user_vec = LeakyReLU(alpha = leaky_alpha)\n",
    "        \n",
    "        self.mlp_item_vec = Dense(n_units, kernel_regularizer = regul, name = 'mlp_item_vec') #\n",
    "        self.leaky_mlp_item_vec = LeakyReLU(alpha = leaky_alpha)\n",
    "\n",
    "        self.concat_mlp_layer = keras.layers.Concatenate(name = 'concat_mlp_layer')\n",
    "\n",
    "        self.gmf_layer = GMFLayer(name = 'gmf_layer')\n",
    "        self.mlp_block = DenseBlock(n_dense_block, n_units, regularizer = regul, name = 'mlp_block')\n",
    "        self.rating_output = Dense(1, activation = sigmoid_1_5, kernel_regularizer = regul, name = 'output')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        user_input, item_input = inputs        \n",
    "        mlp_user_vec = self.mlp_user_vec(user_input)\n",
    "        mlp_user_vec_act = self.leaky_mlp_user_vec(mlp_user_vec)\n",
    "        \n",
    "        mlp_item_vec = self.mlp_item_vec(item_input)\n",
    "        mlp_item_vec_act = self.leaky_mlp_item_vec(mlp_item_vec)\n",
    "        \n",
    "        concat_mlp_layer = self.concat_mlp_layer([mlp_user_vec_act, mlp_item_vec_act])\n",
    "        mlp_block = self.mlp_block(concat_mlp_layer)\n",
    "        rating_output = self.rating_output(mlp_block)\n",
    "        return rating_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_alpha = 0.4\n",
    "reg = keras.regularizers.l2(0.1)\n",
    "opt = keras.optimizers.Adam(0.0001)\n",
    "\n",
    "model = NeuCFModel(name = 'enc_model', n_dense_block = 5, n_units = 30, \n",
    "                   regul = reg, leaky_apha = leaky_alpha)\n",
    "\n",
    "training_loop(model, matrix_sub, validation_set_sub, optimizer = opt, n_epochs = 20)\n",
    "# Result: effective at learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
